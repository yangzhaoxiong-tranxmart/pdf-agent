import streamlit as st
import PyPDF2
from collections import Counter
import matplotlib.pyplot as plt
import base64
from PyPDF2 import PdfReader, PdfWriter
import pymupdf
import tempfile
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import threading
import queue
import torch

# è‡ªå®šä¹‰Streamerç±»ç”¨äºStreamlitæµå¼è¾“å‡º
class StreamlitStreamer(TextStreamer):
    def __init__(self, tokenizer, text_container, skip_special_tokens=True, **decode_kwargs):
        super().__init__(tokenizer, skip_special_tokens=skip_special_tokens, **decode_kwargs)
        self.text_container = text_container
        self.current_text = ""
        self.skip_special_tokens = skip_special_tokens
        self.input_length = 0  # è®°å½•è¾“å…¥promptçš„é•¿åº¦

    def put(self, value):
        """é‡å†™putæ–¹æ³•ä»¥å®æ—¶æ›´æ–°Streamlitç•Œé¢ï¼Œåªæ˜¾ç¤ºæ–°ç”Ÿæˆçš„token"""
        if value is not None:
            # å°†token IDè§£ç ä¸ºæ–‡æœ¬
            if hasattr(value, 'item') and value.numel() == 1:  # å¦‚æœæ˜¯å•ä¸ªtokençš„tensor
                token_id = value.item()
            elif hasattr(value, 'tolist'):  # å¦‚æœæ˜¯åŒ…å«å¤šä¸ªtokençš„tensor
                token_ids = value.tolist()
                if isinstance(token_ids, list) and len(token_ids) > 0 and isinstance(token_ids[0], list):
                    # å¦‚æœtoken_idsæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    token_ids = token_ids[0]
                # è§£ç æ‰€æœ‰tokenä¸ºæ–‡æœ¬
                decoded_text = self.tokenizer.decode(token_ids, skip_special_tokens=self.skip_special_tokens)
                # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥promptéƒ¨åˆ†ï¼‰
                if len(decoded_text) > self.input_length:
                    new_text = decoded_text[self.input_length:]
                    self.current_text += new_text
                    # å®æ—¶æ›´æ–°æ˜¾ç¤º
                    self.text_container.markdown(self.current_text)
                return
            else:
                token_id = value

            # è§£ç tokenä¸ºæ–‡æœ¬
            decoded_text = self.tokenizer.decode([token_id], skip_special_tokens=self.skip_special_tokens)
            self.current_text += decoded_text

            # å®æ—¶æ›´æ–°æ˜¾ç¤º
            self.text_container.markdown(self.current_text)

    def set_input_length(self, length):
        """è®¾ç½®è¾“å…¥promptçš„é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†"""
        self.input_length = length

    def end(self):
        """ç»“æŸæµå¼è¾“å‡º"""
        pass

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(
    page_title="æ™ºèƒ½PDFé˜…è¯»åŠ©æ‰‹",
    page_icon="ğŸ“š",
    layout="wide"
)

# æå–PDFæ–‡æœ¬
def extract_text_from_pdf(uploaded_file):
    pdf_reader = PyPDF2.PdfReader(uploaded_file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def extract_text_from_pdf_2(uploaded_file):
    doc = pymupdf.open(stream=uploaded_file.getvalue(), filetype='pdf')
    text = ''
    for page in doc:
        text = text + page.get_text()
    doc.close()
    return text

def extract_text_from_pdf_nougat(uploaded_file):
    """
    ä½¿ç”¨ Nougat API æå– PDF æ–‡æœ¬å†…å®¹
    """
    try:
        import subprocess
        import tempfile
        import os

        # åˆ›å»ºä¸´æ—¶è¾“å…¥æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_input:
            tmp_input.write(uploaded_file.getvalue())
            input_path = tmp_input.name

        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        output_dir = tempfile.mkdtemp()

        # è°ƒç”¨ Nougat å‘½ä»¤è¡Œå·¥å…·
        # result = subprocess.run([
        #     'nougat',
        #     input_path,
        #     '--out', output_dir,
        #     '--markdown'  # è¾“å‡ºä¸º Markdown æ ¼å¼
        # ], capture_output=True, text=True)
        process = subprocess.Popen([
            'nougat',
            input_path,
            '--out', output_dir,
            '--markdown',  # è¾“å‡ºä¸º Markdown æ ¼å¼
            # '2>&1',
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        # è¯»å–æ ‡å‡†è¾“å‡º
        for line in iter(process.stdout.readline, ''):
            print(line.rstrip())

        # ç­‰å¾…è¿›ç¨‹å®Œæˆå¹¶è·å–è¿”å›ç 
        returncode = process.wait()

        if returncode == 0:
            # è¯»å–ç”Ÿæˆçš„ Markdown æ–‡ä»¶
            output_file = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + '.mmd')
            if os.path.exists(output_file):
                with open(output_file, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                text = "Nougat å¤„ç†å®Œæˆä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
        else:
            text = f"Nougat å¤„ç†å¤±è´¥: {stderr}"

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(input_path)
        import shutil
        shutil.rmtree(output_dir)

        return text

    except Exception as e:
        st.warning(f"Nougat æå–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {str(e)}")
        return extract_text_from_pdf_2(uploaded_file)

# ç”ŸæˆPDFæ‘˜è¦ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼‰
def generate_summary_streaming(tokenizer, model, text, max_length=500, streamer=None):
    messages = [{"role": "user",
                 "content": f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¸è¶…è¿‡{max_length}å­—ï¼š\n\n{text[:10000]}"}]
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)
    
    # ä½¿ç”¨æµå¼è¾“å‡º
    if streamer:
        # è®¾ç½®è¾“å…¥é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†
        input_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
        streamer.set_input_length(len(input_text))
        outputs = model.generate(**inputs, max_new_tokens=2048, temperature=0.7, do_sample=True, streamer=streamer)
    else:
        outputs = model.generate(**inputs, max_new_tokens=2048, temperature=0.7, do_sample=True)
    
    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

# ç”ŸæˆPDFæ‘˜è¦ï¼ˆéæµå¼ç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
def generate_summary(tokenizer, model, text, max_length=500):
    return generate_summary_streaming(tokenizer, model, text, max_length, streamer=None)

# ä½¿ç”¨AIæå–æ›´ç›¸å…³çš„å…³é”®è¯
# def ai_extract_keywords(text, num_keywords=15):
#     prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{num_keywords}ä¸ªæœ€é‡è¦çš„å…³é”®è¯ã€‚
#     è¦æ±‚ï¼šå…³é”®è¯åº”è¯¥æ˜¯æœ€èƒ½ä»£è¡¨æ–‡æ¡£ä¸»é¢˜å’Œå†…å®¹çš„æœ¯è¯­ã€æ¦‚å¿µæˆ–å®ä½“ã€‚
#     æ–‡æœ¬å†…å®¹ï¼š
#     {text[:3000]}

#     è¯·ä»¥é€—å·åˆ†éš”çš„å½¢å¼è¿”å›å…³é”®è¯ï¼š"""

#     try:
#         response = openai.ChatCompletion.create(
#             model="gpt-3.5-turbo",
#             messages=[
#                 {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…³é”®è¯æå–åŠ©æ‰‹ã€‚"},
#                 {"role": "user", "content": prompt}
#             ],
#             max_tokens=100
#         )
#         keywords = response.choices[0].message.content.strip().split(',')
#         keywords = [keyword.strip() for keyword in keywords if keyword.strip()]
#         return keywords[:num_keywords]
#     except Exception as e:
#         return f"æå–å…³é”®è¯æ—¶å‡ºé”™: {str(e)}"

# åˆ’é‡ç‚¹åŠŸèƒ½
# def highlight_important_points(text, num_points=5):
#     prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{num_points}ä¸ªæœ€é‡è¦çš„è§‚ç‚¹æˆ–ä¿¡æ¯ç‚¹ã€‚
#     æ–‡æœ¬å†…å®¹ï¼š
#     {text[:3000]}

#     è¯·ä»¥æ¸…æ™°çš„åˆ—è¡¨å½¢å¼è¿”å›è¿™äº›é‡ç‚¹ï¼š"""

#     try:
#         response = openai.ChatCompletion.create(
#             model="gpt-3.5-turbo",
#             messages=[
#                 {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿æå–å…³é”®ä¿¡æ¯ã€‚"},
#                 {"role": "user", "content": prompt}
#             ],
#             max_tokens=300
#         )
#         return response.choices[0].message.content
#     except Exception as e:
#         return f"åˆ’é‡ç‚¹æ—¶å‡ºé”™: {str(e)}"

# ç”Ÿæˆè¯äº‘
# def generate_wordcloud(keywords):
#     # å‡†å¤‡è¯äº‘æ•°æ®
#     word_freq = {word: count for word, count in keywords}

#     # ç”Ÿæˆè¯äº‘
#     wordcloud = WordCloud(
#         width=800,
#         height=400,
#         background_color='white',
#         colormap='viridis'
#     ).generate_from_frequencies(word_freq)

#     # æ˜¾ç¤ºè¯äº‘
#     plt.figure(figsize=(10, 5))
#     plt.imshow(wordcloud, interpolation='bilinear')
#     plt.axis('off')
#     plt.tight_layout()

#     return plt

# åœ¨PDFä¸­æ·»åŠ é«˜äº®
# def add_highlight_to_pdf(input_pdf, output_pdf, highlights):
#     """
#     ä½¿ç”¨PyMuPDFåœ¨PDFä¸­æ·»åŠ é«˜äº®æ ‡è®°
#     """
#     # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
#     with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
#         tmp_file.write(input_pdf.getvalue())
#         tmp_path = tmp_file.name

#     # æ‰“å¼€PDFæ–‡æ¡£
#     doc = fitz.open(tmp_path)

#     # å¯¹æ¯ä¸ªé«˜äº®é¡¹è¿›è¡Œå¤„ç†
#     for highlight in highlights:
#         page_num = highlight.get("page", 0)
#         text = highlight.get("text", "")
#         color = highlight.get("color", yellow)

#         if page_num < len(doc):
#             page = doc[page_num]

#             # æŸ¥æ‰¾æ–‡æœ¬ä½ç½®
#             text_instances = page.search_for(text)

#             # å¯¹æ¯ä¸ªæ‰¾åˆ°çš„æ–‡æœ¬å®ä¾‹æ·»åŠ é«˜äº®
#             for inst in text_instances:
#                 highlight_annot = page.add_highlight_annot(inst)
#                 highlight_annot.set_colors(stroke=color)
#                 highlight_annot.update()

#     # ä¿å­˜ä¿®æ”¹åçš„PDF
#     doc.save(output_pdf)
#     doc.close()

#     # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
#     os.unlink(tmp_path)

#     return output_pdf

# åœ¨PDFä¸­æ·»åŠ æ³¨é‡Š
# def add_comment_to_pdf(input_pdf, output_pdf, comments):
#     """
#     ä½¿ç”¨PyMuPDFåœ¨PDFä¸­æ·»åŠ æ³¨é‡Š
#     """
#     # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
#     with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
#         tmp_file.write(input_pdf.getvalue())
#         tmp_path = tmp_file.name

#     # æ‰“å¼€PDFæ–‡æ¡£
#     doc = fitz.open(tmp_path)

#     # å¯¹æ¯ä¸ªæ³¨é‡Šé¡¹è¿›è¡Œå¤„ç†
#     for comment in comments:
#         page_num = comment.get("page", 0)
#         text = comment.get("text", "")
#         position = comment.get("position", (50, 50))

#         if page_num < len(doc):
#             page = doc[page_num]

#             # æ·»åŠ æ–‡æœ¬æ³¨é‡Š
#             annot = page.add_text_annot(position, text)
#             annot.set_info(title="AIæ³¨é‡Š", content=text)
#             annot.update()

#     # ä¿å­˜ä¿®æ”¹åçš„PDF
#     doc.save(output_pdf)
#     doc.close()

#     # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
#     os.unlink(tmp_path)

#     return output_pdf

# åˆ›å»ºä¸‹è½½é“¾æ¥
def create_download_link(pdf_data, filename):
    b64 = base64.b64encode(pdf_data).decode()
    href = f'<a href="data:application/pdf;base64,{b64}" download="{filename}">ä¸‹è½½ä¿®æ”¹åçš„PDF</a>'
    return href

# ä¸»åº”ç”¨
def main():
    # æ£€æµ‹è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    st.info(f"ä½¿ç”¨è®¾å¤‡: {device.upper()}")

    tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
    model = AutoModelForCausalLM.from_pretrained(
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,  # GPUä½¿ç”¨åŠç²¾åº¦ï¼ŒCPUä½¿ç”¨å…¨ç²¾åº¦
        device_map="auto" if device == "cuda" else None  # GPUè‡ªåŠ¨åˆ†é…ï¼ŒCPUä¸ä½¿ç”¨
    )
    # å¦‚æœä½¿ç”¨CPUï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CPU
    if device == "cpu":
        model = model.to(device)

    st.title("ğŸ“š æ™ºèƒ½PDFé˜…è¯»åŠ©æ‰‹")
    st.markdown("ä¸Šä¼ PDFæ–‡ä»¶ï¼Œä½¿ç”¨AIè¾…åŠ©é˜…è¯»ã€ç†è§£å’Œåˆ†ææ–‡æ¡£å†…å®¹ï¼Œå¹¶ç›´æ¥åœ¨PDFä¸Šé«˜äº®å’Œæ³¨é‡Š")

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")
    print(uploaded_file)
    print(type(uploaded_file))

    if uploaded_file is not None:
        # é€‰æ‹©æå–æ–¹æ³•
        extraction_method = st.selectbox(
            "é€‰æ‹©æ–‡æœ¬æå–æ–¹æ³•",
            ["PyMuPDF (å¿«é€Ÿ)", "Nougat (ç²¾ç¡®ï¼Œé€‚åˆæ•°å­¦å…¬å¼)"],
            help="Nougat æ›´é€‚åˆåŒ…å«æ•°å­¦å…¬å¼çš„å­¦æœ¯æ–‡æ¡£"
        )
        # æå–æ–‡æœ¬
        with st.spinner("æ­£åœ¨è§£æPDF..."):
            if extraction_method == "Nougat (ç²¾ç¡®ï¼Œé€‚åˆæ•°å­¦å…¬å¼)":
                text = extract_text_from_pdf_nougat(uploaded_file)
            else:
                text = extract_text_from_pdf_2(uploaded_file)

        if not text.strip():
            st.error("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶æˆ–å›¾åƒPDF")
            return

        # åˆ›å»ºé€‰é¡¹å¡
        # tab1, tab2, tab3, tab4, tab5 = st.tabs(["æ–‡æ¡£æ¦‚è§ˆ", "å…³é”®è¯åˆ†æ", "é‡ç‚¹æå–", "æ™ºèƒ½é—®ç­”", "PDFé«˜äº®ä¸æ³¨é‡Š"])
        tab1, = st.tabs(["æ–‡æ¡£æ¦‚è§ˆ"])

        with tab1:
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
                st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
                st.write(f"**é¡µæ•°:** {len(PyPDF2.PdfReader(uploaded_file).pages)}")
                st.write(f"**æ–‡æœ¬é•¿åº¦:** {len(text)} å­—ç¬¦")

            # ç”Ÿæˆæ‘˜è¦
            with col2:
                st.subheader("ğŸ“ æ–‡æ¡£æ‘˜è¦")
                if st.button("ç”Ÿæˆæ‘˜è¦", key="summary_btn"):
                    # åˆ›å»ºç”¨äºæ˜¾ç¤ºæµå¼è¾“å‡ºçš„å®¹å™¨
                    summary_container = st.empty()
                    summary_container.markdown("æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
                    
                    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
                    streamer = StreamlitStreamer(tokenizer, summary_container)
                    
                    # ä½¿ç”¨æµå¼è¾“å‡ºç”Ÿæˆæ‘˜è¦
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ‘˜è¦..."):
                        summary = generate_summary_streaming(tokenizer, model, text, streamer=streamer)
                    
                    # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´æ‘˜è¦
                    summary_container.success("æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
                    st.markdown("**å®Œæ•´æ‘˜è¦ï¼š**")
                    st.markdown(summary)

        # with tab2:
        #     st.subheader("ğŸ”‘ å…³é”®è¯åˆ†æ")

        #     col1, col2 = st.columns(2)

        #     with col1:
        #         if st.button("æå–é«˜é¢‘å…³é”®è¯", key="keywords_btn"):
        #             with st.spinner("æ­£åœ¨åˆ†æå…³é”®è¯..."):
        #                 keywords = extract_keywords(text)
        #                 st.write("**é«˜é¢‘å…³é”®è¯:**")
        #                 for i, (word, count) in enumerate(keywords, 1):
        #                     st.write(f"{i}. {word} (å‡ºç°{count}æ¬¡)")

        #     with col2:
        #         if st.button("AIæå–é‡è¦å…³é”®è¯", key="ai_keywords_btn"):
        #             with st.spinner("AIæ­£åœ¨åˆ†æ..."):
        #                 keywords = ai_extract_keywords(text)
        #                 st.write("**AIæå–çš„å…³é”®è¯:**")
        #                 for i, keyword in enumerate(keywords, 1):
        #                     st.write(f"{i}. {keyword}")

        #     # æ˜¾ç¤ºè¯äº‘
        #     if st.button("ç”Ÿæˆå…³é”®è¯è¯äº‘", key="wordcloud_btn"):
        #         with st.spinner("æ­£åœ¨ç”Ÿæˆè¯äº‘..."):
        #             keywords = extract_keywords(text, 30)
        #             fig = generate_wordcloud(keywords)
        #             st.pyplot(fig)

        # with tab3:
        #     st.subheader("ğŸŒŸ é‡ç‚¹å†…å®¹æå–")

        #     if st.button("åˆ’é‡ç‚¹", key="highlight_btn"):
        #         with st.spinner("AIæ­£åœ¨åˆ†æé‡ç‚¹å†…å®¹..."):
        #             highlights = highlight_important_points(text)
        #             st.success("**æ–‡æ¡£é‡ç‚¹å†…å®¹:**")
        #             st.markdown(highlights)

        #     # å…è®¸ç”¨æˆ·è‡ªå®šä¹‰æœç´¢é‡ç‚¹
        #     st.subheader("ğŸ” è‡ªå®šä¹‰æœç´¢")
        #     search_term = st.text_input("è¾“å…¥æ‚¨å…³æ³¨çš„å…³é”®è¯æˆ–æ¦‚å¿µ:")
        #     if search_term:
        #         # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
        #         sentences = re.split(r'[.!?]+', text)
        #         relevant_sentences = [s.strip() for s in sentences if search_term.lower() in s.lower()]

        #         if relevant_sentences:
        #             st.write(f"æ‰¾åˆ° {len(relevant_sentences)} æ¡ç›¸å…³å†…å®¹:")
        #             for i, sentence in enumerate(relevant_sentences[:10], 1):
        #                 # é«˜äº®æ˜¾ç¤ºæœç´¢è¯
        #                 highlighted = sentence.replace(
        #                     search_term,
        #                     f"<mark>{search_term}</mark>"
        #                 )
        #                 st.markdown(f"{i}. {highlighted}", unsafe_allow_html=True)

        #                 # æ·»åŠ åˆ°é«˜äº®åˆ—è¡¨
        #                 if 'highlights' not in st.session_state:
        #                     st.session_state.highlights = []

        #                 # å°è¯•æ‰¾åˆ°å¥å­æ‰€åœ¨çš„é¡µé¢
        #                 page_num = 0  # é»˜è®¤ç¬¬ä¸€é¡µ
        #                 st.session_state.highlights.append({
        #                     "page": page_num,
        #                     "text": search_term,
        #                     "color": yellow
        #                 })
        #         else:
        #             st.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚")

        # with tab4:
        #     st.subheader("â“ æ™ºèƒ½é—®ç­”")

        #     # åˆ†å‰²æ–‡æœ¬å¹¶åˆ›å»ºå‘é‡å­˜å‚¨
        #     with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å†…å®¹..."):
        #         texts = split_text(text)
        #         vectorstore = create_vector_store(texts)

        #     question = st.text_input("è¾“å…¥å…³äºæ–‡æ¡£çš„é—®é¢˜:")

        #     if question:
        #         with st.spinner("æ€è€ƒä¸­..."):
        #             qa_chain = RetrievalQA.from_chain_type(
        #                 llm=OpenAI(),
        #                 chain_type="stuff",
        #                 retriever=vectorstore.as_retriever(),
        #                 return_source_documents=True
        #             )
        #             result = qa_chain({"query": question})

        #             st.success("**ç­”æ¡ˆ:** " + result["result"])

        #             with st.expander("æŸ¥çœ‹ç›¸å…³æ®µè½"):
        #                 for doc in result["source_documents"]:
        #                     st.write(doc.page_content)
        #                     st.markdown("---")

        # with tab5:
        #     st.subheader("ğŸ–ï¸ PDFé«˜äº®ä¸æ³¨é‡Š")
        #     st.info("åœ¨æ­¤é€‰é¡¹å¡ä¸­ï¼Œæ‚¨å¯ä»¥å°†AIè¯†åˆ«çš„é‡è¦å†…å®¹å’Œæ³¨é‡Šç›´æ¥æ·»åŠ åˆ°PDFæ–‡ä»¶ä¸­")

        #     # é«˜äº®é€‰é¡¹
        #     st.subheader("é«˜äº®é€‰é¡¹")
        #     highlight_color = st.selectbox(
        #         "é€‰æ‹©é«˜äº®é¢œè‰²",
        #         ["é»„è‰²", "çº¢è‰²", "è“è‰²", "ç»¿è‰²"],
        #         key="highlight_color"
        #     )

        #     color_map = {
        #         "é»„è‰²": yellow,
        #         "çº¢è‰²": (1, 0, 0),
        #         "è“è‰²": (0, 0, 1),
        #         "ç»¿è‰²": (0, 1, 0)
        #     }

        #     # æ³¨é‡Šé€‰é¡¹
        #     st.subheader("æ·»åŠ æ³¨é‡Š")
        #     comment_text = st.text_area("è¾“å…¥æ³¨é‡Šå†…å®¹:", height=100)
        #     comment_page = st.number_input("æ³¨é‡Šæ‰€åœ¨é¡µé¢:", min_value=1, value=1) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•

        #     if st.button("æ·»åŠ æ³¨é‡Šåˆ°PDF", key="add_comment_btn"):
        #         if comment_text:
        #             if 'comments' not in st.session_state:
        #                 st.session_state.comments = []

        #             st.session_state.comments.append({
        #                 "page": comment_page,
        #                 "text": comment_text,
        #                 "position": (50, 50)  # é»˜è®¤ä½ç½®ï¼Œå¯ä»¥æ‰©å±•ä¸ºè®©ç”¨æˆ·é€‰æ‹©ä½ç½®
        #             })

        #             st.success("æ³¨é‡Šå·²æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ï¼")
        #         else:
        #             st.warning("è¯·è¾“å…¥æ³¨é‡Šå†…å®¹")

        #     # åº”ç”¨é«˜äº®å’Œæ³¨é‡Šåˆ°PDF
        #     if st.button("ç”Ÿæˆå¸¦é«˜äº®å’Œæ³¨é‡Šçš„PDF", key="apply_highlights_btn"):
        #         with st.spinner("æ­£åœ¨å¤„ç†PDF..."):
        #             # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
        #             with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_output:
        #                 output_path = tmp_output.name

        #             # åº”ç”¨é«˜äº®
        #             if 'highlights' in st.session_state and st.session_state.highlights:
        #                 # æ›´æ–°é«˜äº®é¢œè‰²
        #                 for highlight in st.session_state.highlights:
        #                     highlight["color"] = color_map[highlight_color]

        #                 add_highlight_to_pdf(uploaded_file, output_path, st.session_state.highlights)

        #             # åº”ç”¨æ³¨é‡Š
        #             if 'comments' in st.session_state and st.session_state.comments:
        #                 add_comment_to_pdf(uploaded_file, output_path, st.session_state.comments)

        #             # è¯»å–å¤„ç†åçš„PDF
        #             with open(output_path, "rb") as f:
        #                 pdf_data = f.read()

        #             # åˆ›å»ºä¸‹è½½é“¾æ¥
        #             href = create_download_link(pdf_data, f"annotated_{uploaded_file.name}")
        #             st.markdown(href, unsafe_allow_html=True)

        #             # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        #             os.unlink(output_path)

        # æ˜¾ç¤ºåŸå§‹æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        with st.expander("æŸ¥çœ‹æå–çš„æ–‡æœ¬"):
            st.text_area("æ–‡æœ¬å†…å®¹", text, height=300)

if __name__ == "__main__":
    main()
