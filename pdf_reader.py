import streamlit as st
import PyPDF2
from collections import Counter
import matplotlib.pyplot as plt
import base64
from PyPDF2 import PdfReader, PdfWriter
import pymupdf
import tempfile
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from transformers import BitsAndBytesConfig
import threading
import queue
import torch

# æ£€æµ‹è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"

MAX_TEXT_INPUT_LENGTH = 100000
MAX_OUTPUT_TOKEN = 16384

# è‡ªå®šä¹‰Streamerç±»ç”¨äºStreamlitæµå¼è¾“å‡º
class StreamlitStreamer(TextStreamer):
    def __init__(self, tokenizer, text_container, skip_special_tokens=True, **decode_kwargs):
        super().__init__(tokenizer, skip_special_tokens=skip_special_tokens, **decode_kwargs)
        self.text_container = text_container
        self.current_text = ""
        self.skip_special_tokens = skip_special_tokens
        self.input_length = 0  # è®°å½•è¾“å…¥promptçš„é•¿åº¦

    def put(self, value):
        """é‡å†™putæ–¹æ³•ä»¥å®æ—¶æ›´æ–°Streamlitç•Œé¢ï¼Œåªæ˜¾ç¤ºæ–°ç”Ÿæˆçš„token"""
        if value is not None:
            # å°†token IDè§£ç ä¸ºæ–‡æœ¬
            if hasattr(value, 'item') and value.numel() == 1:  # å¦‚æœæ˜¯å•ä¸ªtokençš„tensor
                token_id = value.item()
            elif hasattr(value, 'tolist'):  # å¦‚æœæ˜¯åŒ…å«å¤šä¸ªtokençš„tensor
                token_ids = value.tolist()
                if isinstance(token_ids, list) and len(token_ids) > 0 and isinstance(token_ids[0], list):
                    # å¦‚æœtoken_idsæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    token_ids = token_ids[0]
                # è§£ç æ‰€æœ‰tokenä¸ºæ–‡æœ¬
                decoded_text = self.tokenizer.decode(token_ids, skip_special_tokens=self.skip_special_tokens)
                # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥promptéƒ¨åˆ†ï¼‰
                if len(decoded_text) > self.input_length:
                    new_text = decoded_text[self.input_length:]
                    self.current_text += new_text
                    self.current_text = self.current_text.replace('\\[', '$$')
                    self.current_text = self.current_text.replace('\\]', '$$')
                    self.current_text = self.current_text.replace('\\(', '$')
                    self.current_text = self.current_text.replace('\\)', '$')
                    # å®æ—¶æ›´æ–°æ˜¾ç¤º
                    self.text_container.markdown(self.current_text)
                return
            else:
                token_id = value

            # è§£ç tokenä¸ºæ–‡æœ¬
            decoded_text = self.tokenizer.decode([token_id], skip_special_tokens=self.skip_special_tokens)
            self.current_text += decoded_text
            self.current_text = self.current_text.replace('\\[', '$$')
            self.current_text = self.current_text.replace('\\]', '$$')
            self.current_text = self.current_text.replace('\\(', '$')
            self.current_text = self.current_text.replace('\\)', '$')

            # å®æ—¶æ›´æ–°æ˜¾ç¤º
            self.text_container.markdown(self.current_text)

    def set_input_length(self, length):
        """è®¾ç½®è¾“å…¥promptçš„é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†"""
        self.input_length = length

    def end(self):
        """ç»“æŸæµå¼è¾“å‡º"""
        pass

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(
    page_title="è®ºæ–‡é˜…è¯»åŠ©æ‰‹",
    page_icon="ğŸ“š",
    layout="wide"
)

# æå–PDFæ–‡æœ¬
def extract_text_from_pdf(uploaded_file):
    pdf_reader = PyPDF2.PdfReader(uploaded_file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def extract_text_from_pdf_2(uploaded_file):
    doc = pymupdf.open(stream=uploaded_file.getvalue(), filetype='pdf')
    text = ''
    for page in doc:
        text = text + page.get_text()
    doc.close()
    return text

def extract_text_from_pdf_nougat(uploaded_file):
    """
    ä½¿ç”¨ Nougat API æå– PDF æ–‡æœ¬å†…å®¹
    """
    try:
        import subprocess
        import tempfile
        import os

        # åˆ›å»ºä¸´æ—¶è¾“å…¥æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_input:
            tmp_input.write(uploaded_file.getvalue())
            input_path = tmp_input.name

        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        output_dir = tempfile.mkdtemp()

        # è°ƒç”¨ Nougat å‘½ä»¤è¡Œå·¥å…·
        # result = subprocess.run([
        #     'nougat',
        #     input_path,
        #     '--out', output_dir,
        #     '--markdown'  # è¾“å‡ºä¸º Markdown æ ¼å¼
        # ], capture_output=True, text=True)
        process = subprocess.Popen([
            'nougat',
            input_path,
            '--out', output_dir,
            '--model', '0.1.0-base',
            '--no-skipping',
            '--markdown',  # è¾“å‡ºä¸º Markdown æ ¼å¼
            # '2>&1',
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        # è¯»å–æ ‡å‡†è¾“å‡º
        # for line in iter(process.stdout.readline, ''):
        #     print(line.rstrip())
        for line in iter(process.stderr.readline, ''):
            print(line.rstrip())

        # ç­‰å¾…è¿›ç¨‹å®Œæˆå¹¶è·å–è¿”å›ç 
        returncode = process.wait()

        if returncode == 0:
            # è¯»å–ç”Ÿæˆçš„ Markdown æ–‡ä»¶
            output_file = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + '.mmd')
            if os.path.exists(output_file):
                with open(output_file, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                text = "Nougat å¤„ç†å®Œæˆä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
        else:
            # TODO(zhaoxiong): add error message
            text = f"Nougat å¤„ç†å¤±è´¥: "

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(input_path)
        import shutil
        shutil.rmtree(output_dir)

        return text

    except Exception as e:
        st.warning(f"Nougat æå–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {str(e)}")
        return extract_text_from_pdf_2(uploaded_file)

# ç”ŸæˆPDFæ‘˜è¦ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼‰
def generate_summary_streaming(tokenizer, text, max_length=1000, streamer=None):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        torch_dtype=torch.float16,
        device_map="auto",
        quantization_config=quantization_config,
        low_cpu_mem_usage=True,
        # offload_folder="./offload",  # è®¾ç½®å¸è½½æ–‡ä»¶å¤¹
        max_memory={0: "14GB", "cpu": "24GB"}  # é™åˆ¶GPUå’ŒCPUå†…å­˜
    )
    print('Model specification:')
    print(model)
    # å¦‚æœä½¿ç”¨CPUï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CPU
    if device == "cpu":
        return 'GPU unavailable'

    messages = [{"role": "user",
                 "content": f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦(in Chinese)ï¼Œä¸è¶…è¿‡{max_length}å­—ï¼š\n\n{text[:MAX_TEXT_INPUT_LENGTH]}"}]
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # ä½¿ç”¨æµå¼è¾“å‡º
    if streamer:
        # è®¾ç½®è¾“å…¥é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†
        input_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
        streamer.set_input_length(len(input_text))
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True, streamer=streamer)
    else:
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True)

    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

def generate_technical_derivation_streaming(tokenizer, text, max_length=1000, streamer=None):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        torch_dtype=torch.float16,
        device_map="auto",
        quantization_config=quantization_config,
        low_cpu_mem_usage=True,
        # offload_folder="./offload",  # è®¾ç½®å¸è½½æ–‡ä»¶å¤¹
        max_memory={0: "14GB", "cpu": "24GB"}  # é™åˆ¶GPUå’ŒCPUå†…å­˜
    )
    print('Model specification:')
    print(model)
    # å¦‚æœä½¿ç”¨CPUï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CPU
    if device == "cpu":
        return 'GPU unavailable'

    # messages = [{"role": "user",
    #              "content": f"è¯·ä»‹ç»è¿™ä¸ªè®ºæ–‡é‡Œçš„ä¸»è¦ç†è®ºæŠ€æœ¯å†…å®¹ï¼Œç»™å‡ºå…³é”®å…¬å¼ï¼Œå¹¶è¿›è¡Œæ¨å¯¼å’Œåˆ†æï¼ˆin Chineseï¼‰ï¼š\n\n{text[:MAX_TEXT_INPUT_LENGTH]}"}]
    messages = [{"role": "user",
                 "content": f"è¯·ç”¨æ•°å­¦çš„è¯­è¨€ä»‹ç»è¿™ä¸ªè®ºæ–‡é‡Œçš„ä¸»è¦ç†è®ºæŠ€æœ¯å†…å®¹ï¼Œç»™å‡ºè¿›è¡Œæ¨å¯¼å’Œåˆ†æï¼š\n\n{text[:MAX_TEXT_INPUT_LENGTH]}"}]
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # ä½¿ç”¨æµå¼è¾“å‡º
    if streamer:
        # è®¾ç½®è¾“å…¥é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†
        input_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
        streamer.set_input_length(len(input_text))
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True, streamer=streamer)
    else:
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True)

    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

def generate_reproduction_code_streaming(tokenizer, text, max_length=1000, streamer=None):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        torch_dtype=torch.float16,
        device_map="auto",
        quantization_config=quantization_config,
        low_cpu_mem_usage=True,
        # offload_folder="./offload",  # è®¾ç½®å¸è½½æ–‡ä»¶å¤¹
        max_memory={0: "14GB", "cpu": "24GB"}  # é™åˆ¶GPUå’ŒCPUå†…å­˜
    )
    print('Model specification:')
    print(model)
    # å¦‚æœä½¿ç”¨CPUï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CPU
    if device == "cpu":
        return 'GPU unavailable'

    messages = [{"role": "user",
                 "content": f"è¯·æ ¹æ®è¿™ç¯‡è®ºæ–‡é‡Œçš„ç†è®ºæŠ€æœ¯å†…å®¹ï¼Œç”Ÿæˆä¸€æ®µå¤ç°çš„ä»£ç ï¼š\n\n{text[:MAX_TEXT_INPUT_LENGTH]}"}]
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # ä½¿ç”¨æµå¼è¾“å‡º
    if streamer:
        # è®¾ç½®è¾“å…¥é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ‰promptéƒ¨åˆ†
        input_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
        streamer.set_input_length(len(input_text))
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True, streamer=streamer)
    else:
        outputs = model.generate(**inputs, max_new_tokens=MAX_OUTPUT_TOKEN, temperature=0.7, do_sample=True)

    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

# ç”ŸæˆPDFæ‘˜è¦ï¼ˆéæµå¼ç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
def generate_summary(tokenizer, text, max_length=500):
    return generate_summary_streaming(tokenizer, text, max_length, streamer=None)


# åˆ›å»ºä¸‹è½½é“¾æ¥
def create_download_link(pdf_data, filename):
    b64 = base64.b64encode(pdf_data).decode()
    href = f'<a href="data:application/pdf;base64,{b64}" download="{filename}">ä¸‹è½½ä¿®æ”¹åçš„PDF</a>'
    return href

def main():
    st.info(f"ä½¿ç”¨è®¾å¤‡: {device.upper()}")

    tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-14B")

    st.title("ğŸ“š æ™ºèƒ½PDFé˜…è¯»åŠ©æ‰‹")
    st.markdown("ä¸Šä¼ PDFæ–‡ä»¶ï¼Œä½¿ç”¨AIè¾…åŠ©é˜…è¯»ã€ç†è§£å’Œåˆ†ææ–‡æ¡£å†…å®¹ï¼Œå¹¶ç›´æ¥åœ¨PDFä¸Šé«˜äº®å’Œæ³¨é‡Š")

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")
    print('Uploaded file: ', uploaded_file)
    # print(type(uploaded_file))

    text = ''

    # åœ¨æ–‡ä»¶ä¸Šä¼ åæ£€æŸ¥æ˜¯å¦å·²ç»æå–è¿‡æ–‡æœ¬
    if uploaded_file is not None:
        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºç¼“å­˜é”®
        cache_key = f"text_{uploaded_file.name}"
        extraction_key = f"extraction_method_{uploaded_file.name}"
        
        # æ€»æ˜¯æ˜¾ç¤ºæå–æ–¹æ³•é€‰æ‹©æ¡†ï¼Œä½†è®°ä½ç”¨æˆ·çš„é€‰æ‹©
        if extraction_key not in st.session_state:
            st.session_state[extraction_key] = "PyMuPDF (å¿«é€Ÿ)"
        
        extraction_method = st.selectbox(
            "é€‰æ‹©æ–‡æœ¬æå–æ–¹æ³•",
            ["PyMuPDF (å¿«é€Ÿ)", "Nougat (ç²¾ç¡®ï¼Œé€‚åˆæ•°å­¦å…¬å¼)"],
            index=0 if st.session_state[extraction_key] == "PyMuPDF (å¿«é€Ÿ)" else 1,
            help="Nougat æ›´é€‚åˆåŒ…å«æ•°å­¦å…¬å¼çš„å­¦æœ¯æ–‡æ¡£"
        )
        
        # æ›´æ–°é€‰æ‹©çš„æ–¹æ³•
        st.session_state[extraction_key] = extraction_method
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æå–æ–‡æœ¬
        need_extraction = (
            cache_key not in st.session_state or 
            st.session_state.get(f"last_method_{uploaded_file.name}") != extraction_method
        )
        
        if need_extraction:
            with st.spinner("æ­£åœ¨è§£æPDF..."):
                if extraction_method == "Nougat (ç²¾ç¡®ï¼Œé€‚åˆæ•°å­¦å…¬å¼)":
                    extracted_text = extract_text_from_pdf_nougat(uploaded_file)
                else:
                    extracted_text = extract_text_from_pdf_2(uploaded_file)
                
                # å­˜å‚¨æå–çš„æ–‡æœ¬å’Œä½¿ç”¨çš„æå–æ–¹æ³•
                st.session_state[cache_key] = extracted_text
                st.session_state[f"last_method_{uploaded_file.name}"] = extraction_method
        
        text = st.session_state[cache_key]

        if not text.strip():
            st.error("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶æˆ–å›¾åƒPDF")
            return

        # åˆ›å»ºé€‰é¡¹å¡
        tab1, tab2, tab3 = st.tabs(["æ–‡æ¡£æ¦‚è§ˆ", "å…³é”®æ¨å¯¼", "Talk is cheap"])

        with tab1:
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
                st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
                # é¿å…é‡å¤åˆ›å»º PdfReader
                if 'pdf_pages' not in st.session_state:
                    st.session_state['pdf_pages'] = len(PyPDF2.PdfReader(uploaded_file).pages)
                st.write(f"**é¡µæ•°:** {st.session_state['pdf_pages']}")
                st.write(f"**æ–‡æœ¬é•¿åº¦:** {len(text)} å­—ç¬¦")

            # ç”Ÿæˆæ‘˜è¦
            with col2:
                st.subheader("ğŸ“ æ–‡æ¡£æ‘˜è¦")
                if st.button("ç”Ÿæˆæ‘˜è¦", key="summary_btn"):
                    # åˆ›å»ºç”¨äºæ˜¾ç¤ºæµå¼è¾“å‡ºçš„å®¹å™¨
                    summary_container = st.empty()
                    # summary_container.markdown("æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")

                    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
                    streamer = StreamlitStreamer(tokenizer, summary_container)

                    # ä½¿ç”¨æµå¼è¾“å‡ºç”Ÿæˆæ‘˜è¦
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ‘˜è¦..."):
                        summary = generate_summary_streaming(tokenizer, text, streamer=streamer)

                    # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´æ‘˜è¦
                    summary_container.success("æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
                    st.markdown("**å®Œæ•´æ‘˜è¦ï¼š**")
                    st.markdown(summary)

        with tab2:
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
                st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
                # é¿å…é‡å¤åˆ›å»º PdfReader
                if 'pdf_pages' not in st.session_state:
                    st.session_state['pdf_pages'] = len(PyPDF2.PdfReader(uploaded_file).pages)
                st.write(f"**é¡µæ•°:** {st.session_state['pdf_pages']}")
                st.write(f"**æ–‡æœ¬é•¿åº¦:** {len(text)} å­—ç¬¦")

            with col2:
                st.subheader("ğŸ“ å…³é”®æ¨å¯¼")
                if st.button("ç”Ÿæˆå…³é”®æ¨å¯¼", key="derivation_btn"):
                    # åˆ›å»ºç”¨äºæ˜¾ç¤ºæµå¼è¾“å‡ºçš„å®¹å™¨
                    summary_container = st.empty()

                    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
                    streamer = StreamlitStreamer(tokenizer, summary_container)

                    # ä½¿ç”¨æµå¼è¾“å‡ºç”Ÿæˆæ‘˜è¦
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨å¯¼..."):
                        summary = generate_technical_derivation_streaming(tokenizer, text, streamer=streamer)

                    # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´æ‘˜è¦
                    summary_container.success("æ¨å¯¼ç”Ÿæˆå®Œæˆï¼")
                    st.markdown("**å…³é”®æ¨å¯¼ï¼š**")
                    summary = summary.replace('\\[', '$$')
                    summary = summary.replace('\\]', '$$')
                    summary = summary.replace('\\(', '$')
                    summary = summary.replace('\\)', '$')
                    st.markdown(summary)
                    # st.text(summary)

        with tab3:
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
                st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
                # é¿å…é‡å¤åˆ›å»º PdfReader
                if 'pdf_pages' not in st.session_state:
                    st.session_state['pdf_pages'] = len(PyPDF2.PdfReader(uploaded_file).pages)
                st.write(f"**é¡µæ•°:** {st.session_state['pdf_pages']}")
                st.write(f"**æ–‡æœ¬é•¿åº¦:** {len(text)} å­—ç¬¦")

            with col2:
                st.subheader("ğŸ’» ä»£ç å¤ç°")
                summary_container_1 = st.empty()
                summary_container_1.markdown("çœ‹ä¸æ‡‚ï¼Œç›´æ¥å¸®æˆ‘æŠŠä»£ç å†™äº†å§")
                if st.button("ç”Ÿæˆä»£ç ", key="reproduction_btn"):
                    summary_container = st.empty()

                    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
                    streamer = StreamlitStreamer(tokenizer, summary_container)

                    # ä½¿ç”¨æµå¼è¾“å‡ºç”Ÿæˆæ‘˜è¦
                    with st.spinner("æ­£åœ¨ç”Ÿæˆä»£ç ..."):
                        summary = generate_reproduction_code_streaming(tokenizer, text, streamer=streamer)

                    # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´æ‘˜è¦
                    summary_container.success("ä»£ç ç”Ÿæˆå®Œæˆï¼")
                    st.markdown("**å¤ç°ä»£ç ï¼š**")
                    st.markdown(summary)

        # æ˜¾ç¤ºåŸå§‹æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        with st.expander("æŸ¥çœ‹æå–çš„æ–‡æœ¬"):
            st.text_area("æ–‡æœ¬å†…å®¹", text, height=300)

if __name__ == "__main__":
    main()
